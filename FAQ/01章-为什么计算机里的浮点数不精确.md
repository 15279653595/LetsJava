首先一个事实是，计算机是用固定的字节数来表示一个浮点数的。我们就拿double来举例，一个 double 变量占用8个字节，和整数的 long 是一样的。

**但是小数的可能性有多少呢？从0.1到0.2，就有无数个小数。所以，浮点数不可能完全精确的表示小数。**

在这里我大概说一下计算机是如何让处理小数的。怎么用有限的存储，表示无限的小数呢？原理很简单，就是舍弃精度。

比如说（没有写程序验证，只是从道理上说）对于从6.0000000000000001到6.0000000000000008的小数，计算机是用同一个二进制的数字来表示的，同时，计算和显示也都是用6.0000000000000001。这样就相当于舍弃了精度，让浮点数可以“近似的”表示很大范围内的浮点数。

当然，如果我们要表示整数部分很大的数字，比如123456789987654321.000009，那么精度将会变得更低。

所以大家理解为什么叫做浮点数了吗？因为浮点数的这个点，是指小数点；浮，是指这个小数点会浮动。如果整数部分过大，那么小数点后面的位数和精度就会变小。舍弃小数精度，让值更接近像表示的值。

其实浮点数在计算机里，是一个大学问。最开始PC上的CPU，是不能从硬件层面支持浮点数计算的，都要靠软件模拟，速度非常慢。当时牛X的PC，会带一个浮点数的协处理器，专门用来从硬件层面支持浮点数的运算。如果想深入了解为什么浮点数这么复杂，请参考下文：

[What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html)
